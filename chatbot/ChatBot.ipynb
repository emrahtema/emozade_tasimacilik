{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emrah.tema/.virtualenvs/jupyter/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/emrah.tema/.virtualenvs/jupyter/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/emrah.tema/.virtualenvs/jupyter/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/emrah.tema/.virtualenvs/jupyter/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/emrah.tema/.virtualenvs/jupyter/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/emrah.tema/.virtualenvs/jupyter/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/emrah.tema/.virtualenvs/jupyter/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/emrah.tema/.virtualenvs/jupyter/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/emrah.tema/.virtualenvs/jupyter/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/emrah.tema/.virtualenvs/jupyter/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/emrah.tema/.virtualenvs/jupyter/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/emrah.tema/.virtualenvs/jupyter/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re # for text processing\n",
    "import time # to see epochs time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data PreProcessing\n",
    "cornell movie dialogs corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumOf sentences in conversations: 304714\n",
      "['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!', 'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!', 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.']\n",
      "NumOf conversations: 83098\n",
      "[\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\", \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\", \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\"]\n"
     ]
    }
   ],
   "source": [
    "# importing the dataset\n",
    "lines = open(\"data/movie_lines.txt\", encoding = \"utf-8\", errors = \"ignore\").read().split(\"\\n\")\n",
    "conversations = open(\"data/movie_conversations.txt\", encoding = \"utf-8\", errors = \"ignore\").read().split(\"\\n\")\n",
    "\n",
    "print(\"NumOf sentences in conversations:\", len(lines))\n",
    "print(lines[:3])\n",
    "print(\"NumOf conversations:\", len(conversations))\n",
    "print(conversations[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumOf sentences in conversations: 304713\n",
      "They do not!\n",
      "They do to!\n",
      "I hope so.\n"
     ]
    }
   ],
   "source": [
    "# creating a dictionary that maps each line and its id\n",
    "id2line = dict()\n",
    "for line in lines:\n",
    "    _line = line.split(\" +++$+++ \") # _ means it is temporary variable, will not be used forever\n",
    "    if len(_line) == 5: # if it is not, it is a wrong data\n",
    "        id2line[_line[0]] = _line[4]\n",
    "        # 1,2,3 indexes are metadata, so they are not needed for training, they are extra informations\n",
    "\n",
    "print(\"NumOf sentences in conversations:\", len(id2line))\n",
    "counter = 0\n",
    "for key in id2line.keys():\n",
    "    print(id2line[key])\n",
    "    counter += 1\n",
    "    if counter == 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumOf conversations: 83097\n",
      "[['L194', 'L195', 'L196', 'L197'], ['L198', 'L199'], ['L200', 'L201', 'L202', 'L203']]\n"
     ]
    }
   ],
   "source": [
    "# creating a list of all conversations\n",
    "conversations_ids = list()\n",
    "for conversation in conversations[:-1]: # last row is empty\n",
    "    _conversation = conversation.split(\" +++$+++ \")[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
    "    conversations_ids.append(_conversation.split(\",\"))\n",
    "\n",
    "print(\"NumOf conversations:\", len(conversations_ids))\n",
    "print(conversations_ids[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again. >> Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you. >> Not the hacking and gagging and spitting part.  Please.\n",
      "Not the hacking and gagging and spitting part.  Please. >> Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n"
     ]
    }
   ],
   "source": [
    "# getting seperately the questions and the answers\n",
    "questions = list()\n",
    "answers = list()\n",
    "\n",
    "for conversation in conversations_ids:\n",
    "    for i in range(len(conversation) -1):\n",
    "        questions.append(id2line[conversation[i]])\n",
    "        answers.append(id2line[conversation[i+1]])\n",
    "\n",
    "for i in range(3): print(questions[i], \">>\", answers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumOf cleaned questions, answers: 221616 221616\n",
      "can we make this quick  roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad  again >> well i thought we would start with pronunciation if that is okay with you\n",
      "well i thought we would start with pronunciation if that is okay with you >> not the hacking and gagging and spitting part  please\n",
      "not the hacking and gagging and spitting part  please >> okay then how 'bout we try out some french cuisine  saturday  night\n"
     ]
    }
   ],
   "source": [
    "# doing the first cleaning of the texts\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}+=~|.?,]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "# Cleaning the questions\n",
    "cleaned_questions = list()\n",
    "for question in questions:\n",
    "    cleaned_questions.append(clean_text(question))\n",
    "\n",
    "# Cleaning the answers\n",
    "cleaned_answers = list()\n",
    "for answer in answers:\n",
    "    cleaned_answers.append(clean_text(answer))\n",
    "\n",
    "print(\"NumOf cleaned questions, answers:\", len(cleaned_questions), len(cleaned_answers))\n",
    "for i in range(3): print(cleaned_questions[i], \">>\", cleaned_answers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 76422\n"
     ]
    }
   ],
   "source": [
    "# calculate the words frequencies and delete which are under the treshold\n",
    "word2count = dict()\n",
    "\n",
    "for question in cleaned_questions:\n",
    "    for word in question.split():\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1\n",
    "\n",
    "for answer in cleaned_answers:\n",
    "    for word in answer.split():\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1\n",
    "\n",
    "print(\"Number of words:\", len(word2count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frequent words: 8821\n",
      "can 0\n",
      "we 1\n",
      "make 2\n",
      "this 3\n",
      "quick 4\n"
     ]
    }
   ],
   "source": [
    "# creating two dictionaries that map the question words and the answer words to a unique integer\n",
    "treshold = 20 # higher; reduces training time, lower; increase the overhelming\n",
    "questionswords2int = dict()\n",
    "answerswords2int = dict()\n",
    "word_number = 0\n",
    "for word, count in word2count.items():\n",
    "    if count >= treshold:\n",
    "        questionswords2int[word] = word_number\n",
    "        answerswords2int[word] = word_number\n",
    "        word_number += 1\n",
    "\n",
    "print(\"Number of frequent words:\", len(questionswords2int))\n",
    "counter = 0\n",
    "for key, value in questionswords2int.items():\n",
    "    print(key, value)\n",
    "    counter += 1\n",
    "    if counter == 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the last tokens to the dictionaries, these are neccesary for encoding-decoding layers\n",
    "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>'] # EnfOfString, StartOfString\n",
    "for token in tokens:\n",
    "    questionswords2int[token] = word_number\n",
    "    answerswords2int[token] = word_number\n",
    "\n",
    "# creating the inverse dictionary of answers\n",
    "answersint2word = {val:key for key, val in answerswords2int.items()}\n",
    "\n",
    "# adding the End Of String token to the end of every answer\n",
    "for i in range(len(cleaned_answers)):\n",
    "    cleaned_answers[i] += ' <EOS>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translating all the questions and the answers to integers\n",
    "# and replacing all the words that were filteret out with <OUT>\n",
    "questions_into_int = list()\n",
    "for question in cleaned_questions:\n",
    "    ints = []\n",
    "    for word in question.split():\n",
    "        if word not in questionswords2int:\n",
    "            ints.append(questionswords2int[\"<OUT>\"])\n",
    "        else:\n",
    "            ints.append(questionswords2int[word])\n",
    "    questions_into_int.append(ints)\n",
    "\n",
    "answers_into_int = list()\n",
    "for answer in cleaned_answers:\n",
    "    ints = []\n",
    "    for word in answer.split():\n",
    "        if word not in answerswords2int:\n",
    "            ints.append(answerswords2int[\"<OUT>\"])\n",
    "        else:\n",
    "            ints.append(answerswords2int[word])\n",
    "    answers_into_int.append(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203949 203949\n"
     ]
    }
   ],
   "source": [
    "# sorting questions and answers by the lengt of questions\n",
    "# the purpose is to train the ANN like a human baby\n",
    "# firstly we must teach the sort sentences instead of long sentences for a better train\n",
    "# there are sentences which contains more than 25 words but we don't take them\n",
    "sorted_clean_questions = list()\n",
    "sorted_clean_answers = list()\n",
    "for length in range(1, 25+1): # the sentences with length above 25 are not taken\n",
    "        for i in enumerate(questions_into_int): # enumerate -> tuple; i[0] = index, i[1] = sentence\n",
    "                if len(i[1]) == length:\n",
    "                        sorted_clean_questions.append(questions_into_int[i[0]])\n",
    "                        sorted_clean_answers.append(answers_into_int[i[0]])\n",
    "print(len(sorted_clean_questions), len(sorted_clean_answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture of ChatBot: Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating placeholders for the input and the targets\n",
    "def model_inputs():\n",
    "    # tf.placeholder(dataType, dimensions, )\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name = \"input\")\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name = \"target\")\n",
    "    lr = tf.placeholder(tf.float32, name = \"learning_rate\")\n",
    "    keep_prob = tf.placeholder(tf.float32, name = \"keep_prob\") # controls the dropout rate\n",
    "    return inputs, targets, lr, keep_prob\n",
    "\n",
    "\n",
    "# preprocessing the targets\n",
    "# targets must be batches, bcs decoder accepts targets as batches, doesn't accept a single target\n",
    "def preprocess_targets(targets, word2int, batch_size):\n",
    "    # <SOS>Sentence... == <Left>Right\n",
    "    # tf.fill(dimension, fillWithThis)\n",
    "    left_side = tf.fill([batch_size, 1], word2int[\"<SOS>\"])\n",
    "    # tf.strided_slice(inputs, start, end) take a subset of the data\n",
    "    right_side = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1])\n",
    "    preprocessed_targets = tf.concat([left_side, right_side], 1) # horizontal concat = 1, vertical = 0\n",
    "    return preprocessed_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the encoder RNN layer\n",
    "def encoder_rnn(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length): # seq_len = batch size\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size) # num of input tensors\n",
    "    # DropoutWrapper(ANN which will dropout applied, control the dropout rate)\n",
    "    # %20 percent of neurons are like nonexisting, their weights are not updated\n",
    "    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "    # not the state, encoder cell is composed of several layers\n",
    "    encoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "    _, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = encoder_cell, \n",
    "                                                       cell_bw = encoder_cell, \n",
    "                                                       sequence_length = sequence_length, \n",
    "                                                       inputs = rnn_inputs,\n",
    "                                                       dtype = tf.float32)\n",
    "    return encoder_state\n",
    "\n",
    "\n",
    "# decoding the training set\n",
    "def decode_training_set(encoder_state, decoder_cell, decoder_embedded_input,\n",
    "                       sequence_length, decoding_scope, output_function, keep_prob, batch_size):\n",
    "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size]) # [row_size, col_num, col_size]\n",
    "    # attention = at\n",
    "    at_keys, at_values, at_score_function, at_construct_function = tf.contrib.seq2seq.prepare_attention(\n",
    "        attention_states, attention_option = \"bahdanau\", num_units=decoder_cell.output_size)\n",
    "    training_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_train(\n",
    "        encoder_state[0], at_keys, at_values, at_score_function, at_construct_function, name=\"attn_dec_train\")\n",
    "    decoder_output, _, _, = tf.contrib.seq2seq.dynamic_rnn_decoder(\n",
    "        decoder_cell, training_decoder_function, decoder_embedded_input, sequence_length, scope=decoding_scope)\n",
    "    decoder_output_dropout = tf.nn.dropout(decoder_output, keep_prob)\n",
    "    return output_function(decoder_output_dropout)\n",
    "\n",
    "\n",
    "# decoding the test/validation set\n",
    "# not only predicting the question type, but also create an answer for the question\n",
    "# validation set is being used like cross validation, reduce overfitting and increase the accuracy\n",
    "def decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix, sos_id, eos_id, maximum_length, num_words,\n",
    "                       sequence_length, decoding_scope, output_function, keep_prob, batch_size):\n",
    "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size]) # [row_size, col_num, col_size]\n",
    "    # attention = at\n",
    "    at_keys, at_values, at_score_function, at_construct_function = tf.contrib.seq2seq.prepare_attention(\n",
    "        attention_states, attention_option = \"bahdanau\", num_units=decoder_cell.output_size)\n",
    "    test_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_inference(\n",
    "        output_function, encoder_state[0], at_keys, at_values, at_score_function, at_construct_function,\n",
    "        decoder_embedding_matrix, sos_id, eos_id, maximum_length, num_words, name=\"attn_dec_inf\")\n",
    "    test_predictions, _, _, = tf.contrib.seq2seq.dynamic_rnn_decoder(\n",
    "        decoder_cell, test_decoder_function, scope=decoding_scope)\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the decoder RNN\n",
    "def decoder_rnn(decoder_embedded_input, decoder_embeddings_matrix, encoder_state, num_words, sequence_length,\n",
    "               rnn_size, num_layers, word2int, keep_prob, batch_size):\n",
    "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob=keep_prob)\n",
    "        decoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "        weights = tf.truncated_normal_initializer(stddev = 0.1)\n",
    "        biases = tf.zeros_initializer()\n",
    "        output_function = lambda x: tf.contrib.layers.fully_connected(x, num_words, None, scope=decoding_scope,\n",
    "                                                                     weights_initializer=weights,\n",
    "                                                                     biases_initializer=biases)\n",
    "        training_predictions = decode_training_set(encoder_state, decoder_cell, decoder_embedded_input,\n",
    "                                                  sequence_length, decoding_scope, output_function,\n",
    "                                                  keep_prob, batch_size)\n",
    "        decoding_scope.reused_variables()\n",
    "        test_predictions = decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix,\n",
    "                                          word2int['<SOS>'], word2int['<EOS>'], sequence_length-1,\n",
    "                                          num_words, decoding_scope, output_function, keep_prob, batch_size)\n",
    "        return training_predictions, test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Seq2Seq Model (the brain of network)\n",
    "def seq2seq_model(inputs, targets, keep_prob, batch_size, sequence_length, answers_num_words,\n",
    "                 questions_num_words, encoder_embedding_size, decoder_embedding_size, rnn_size,\n",
    "                 num_layers, questionswords2int):\n",
    "    encoder_embedded_input = tf.contrib.layers.embed_sequence(inputs, answers_num_words+1,\n",
    "                                                             encoder_embedding_size,\n",
    "                                                             initializer = tf.random_uniform_initializer(0, 1))\n",
    "    encoder_state = encoder_rnn(encoder_embedded_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
    "    preprocessed_targets = preprocess_targets(targets, questionswords2int, batch_size)\n",
    "    decoder_embeddings_matrix = tf.Variable(tf.random_uniform([questions_num_words+1, decoder_embedding_size], 0, 1))\n",
    "    decoder_embedded_input = tf.nn.embedding_lookup(decoder_embeddings_matrix, preprocessed_targets)\n",
    "    training_predictions, test_predictions = decoder_rnn(decoder_embedded_input, decoder_embeddings_matrix,\n",
    "                                                        encoder_state, questions_num_words, sequence_length,\n",
    "                                                        rnn_size, num_layers, questionswords2int,\n",
    "                                                        keep_prob, batch_size)\n",
    "    return training_predictions, test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the HyperParameters to make it smart and able to chat (usual-common values are given)\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "rnn_size = 512\n",
    "num_layers = 3\n",
    "encoding_embedding_size = 512\n",
    "decoding_embedding_size = 512\n",
    "learning_rate = 0.01\n",
    "learning_rate_decay = 0.9 # learning rate will be reduced to %90 percent over iterations\n",
    "min_learning_rate = 0.0001 # we dont want the learning_rate goes to the zero\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.contrib.seq2seq' has no attribute 'prepare_attention'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c51c72d3a58a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                                        \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestionswords2int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding_embedding_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                                        \u001b[0mdecoding_embedding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                                                        num_layers, questionswords2int)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-62ea163ffb5d>\u001b[0m in \u001b[0;36mseq2seq_model\u001b[0;34m(inputs, targets, keep_prob, batch_size, sequence_length, answers_num_words, questions_num_words, encoder_embedding_size, decoder_embedding_size, rnn_size, num_layers, questionswords2int)\u001b[0m\n\u001b[1;32m     13\u001b[0m                                                         \u001b[0mencoder_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestions_num_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                                         \u001b[0mrnn_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestionswords2int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                                                         keep_prob, batch_size)\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtraining_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-fede46e4722f>\u001b[0m in \u001b[0;36mdecoder_rnn\u001b[0;34m(decoder_embedded_input, decoder_embeddings_matrix, encoder_state, num_words, sequence_length, rnn_size, num_layers, word2int, keep_prob, batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m         training_predictions = decode_training_set(encoder_state, decoder_cell, decoder_embedded_input,\n\u001b[1;32m     14\u001b[0m                                                   \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoding_scope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                                                   keep_prob, batch_size)\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mdecoding_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreused_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         test_predictions = decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix,\n",
      "\u001b[0;32m<ipython-input-22-d2aa9009d4a3>\u001b[0m in \u001b[0;36mdecode_training_set\u001b[0;34m(encoder_state, decoder_cell, decoder_embedded_input, sequence_length, decoding_scope, output_function, keep_prob, batch_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mattention_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [row_size, col_num, col_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# attention = at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     at_keys, at_values, at_score_function, at_construct_function = tf.contrib.seq2seq.prepare_attention(\n\u001b[0m\u001b[1;32m     23\u001b[0m         attention_states, attention_option = \"bahdanau\", num_units=decoder_cell.output_size)\n\u001b[1;32m     24\u001b[0m     training_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_train(\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.contrib.seq2seq' has no attribute 'prepare_attention'"
     ]
    }
   ],
   "source": [
    "# Defining a session\n",
    "tf.reset_default_graph()\n",
    "session = tf.InteractiveSession()\n",
    " \n",
    "# Loading the model inputs\n",
    "inputs, targets, lr, keep_prob = model_inputs()\n",
    " \n",
    "# Setting the sequence length\n",
    "sequence_length = tf.placeholder_with_default(25, None, name = 'sequence_length') # answer contains max 25 words\n",
    " \n",
    "# Getting the shape of the inputs tensor\n",
    "input_shape = tf.shape(inputs)\n",
    " \n",
    "# Getting the training and test predictions\n",
    "training_predictions, test_predictions = seq2seq_model(tf.reverse(inputs, [-1]), targets, keep_prob, batch_size,\n",
    "                                                       sequence_length, len(answerswords2int),\n",
    "                                                       len(questionswords2int), encoding_embedding_size,\n",
    "                                                       decoding_embedding_size, rnn_size,\n",
    "                                                       num_layers, questionswords2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
