{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOUTUBE VIDEO SCRAPING IN A CHANNEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri Kazıma\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests \n",
    "\n",
    "# Konsol Güzelleştirme\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrap from a link "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_from_url(url):   \n",
    "    html = requests.get(url).text\n",
    "    soup = bs(html, \"html.parser\")         \n",
    "    tags = soup.findAll(\"h3\", class_=\"yt-lockup-title\")\n",
    "    \n",
    "    urls = []\n",
    "    for item in tags:\n",
    "        a = item.findAll(\"a\")[0]\n",
    "        href = \"https://www.youtube.com\" + a[\"href\"]\n",
    "        duration = str(item).split('Duration: ')[1]\n",
    "        duration = duration.split('.</span>')[0]\n",
    "        title = a[\"title\"]\n",
    "        urls.append([href, [href, duration, title]])\n",
    "    return urls\n",
    "\n",
    "\n",
    "def save_scrapped_urls(channels):\n",
    "    for channel in channels:\n",
    "        urls = scrap_from_url(channel[1])\n",
    "        player = channel[0]\n",
    "        \n",
    "        # General records operations\n",
    "        with open(\"scrapped_links/permanents/\" + player + \"/\" + player +\".txt\", \"r\") as general_file:\n",
    "            general_records = general_file.read().split(\"\\n\")\n",
    "        \n",
    "        new_urls = []\n",
    "        for url in urls:\n",
    "            if url[0] not in general_records:\n",
    "                new_urls.append(url)\n",
    "        \n",
    "        new_general_records = [x[0] for x in new_urls]\n",
    "        general_records = general_records + new_general_records\n",
    "        general_records = '\\n'.join(general_records)\n",
    "        \n",
    "        with open(\"scrapped_links/permanents/\" + player + \"/\" + player +\".txt\", \"w\") as general_file:\n",
    "            general_file.write(general_records)\n",
    "        \n",
    "        \n",
    "        with open(\"scrapped_links/permanents/\" + player + \"/\" + player +\"_details.txt\", \"r\") as general_details:\n",
    "            urls_details = general_details.read()\n",
    "        \n",
    "        details = [','.join(x[1]) for x in new_urls]\n",
    "        details = urls_details + \"\\n\" + '\\n'.join(details)\n",
    "        with open(\"scrapped_links/permanents/\" + player + \"/\" + player +\"_details.txt\", \"w\") as details_file:\n",
    "            details_file.write(details)\n",
    "        \n",
    "        # Temporary Records\n",
    "        temporary_urls = '\\n'.join(new_general_records)\n",
    "        date_time = str(datetime.datetime.now())[:16]\n",
    "        with open(\"scrapped_links/temporaries/\" + player + \"/\" + player + date_time + \".txt\", \"w\") as temporary_file:\n",
    "            temporary_file.write(temporary_urls)\n",
    "        \n",
    "        print(len(new_urls), \"tane yeni link kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tane yeni link kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "channels = [\n",
    "    [\"baris\", \"https://www.youtube.com/channel/UCse4cbPiGrSj5qwUV9GVvGw/videos\"],\n",
    "]\n",
    "\n",
    "save_scrapped_urls(channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrap from a html file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_from_html(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        html = file.read()\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    tags = soup.findAll(\"a\", class_=\"yt-simple-endpoint style-scope ytd-grid-video-renderer\")\n",
    "    \n",
    "    urls = []\n",
    "    for item in tags:\n",
    "        href = item[\"href\"].split(\"&\")[0]\n",
    "        title = item[\"title\"]\n",
    "        duration = ' '.join(item[\"aria-label\"].split(\" \")[-4:-2])\n",
    "        urls.append([href, [href, duration, title]])\n",
    "    return urls\n",
    "\n",
    "\n",
    "def save_scrapped_html_urls(player, file_path):\n",
    "    urls = scrap_from_html(file_path)\n",
    "    SAVE_PATH = \"scrapped_links/permanents/\" + player\n",
    "    \n",
    "    general_record = \"\"\n",
    "    details_record = \"\"\n",
    "    for url_item in urls:\n",
    "        general_record += url_item[0] + \"\\n\"\n",
    "        details_record += ','.join(url_item[1]) + \"\\n\"\n",
    "    \n",
    "    with open(SAVE_PATH + \"/\" + player + \".txt\", \"w\") as general_file:\n",
    "        general_file.write(general_record)\n",
    "    \n",
    "    with open(SAVE_PATH + \"/\" + player + \"_details.txt\", \"w\") as details_file:\n",
    "        details_file.write(details_record)\n",
    "    \n",
    "    print(len(urls), \"tane kayıt kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181 tane kayıt kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "save_scrapped_html_urls(\"baris\", \"ytt.htm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
