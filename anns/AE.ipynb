{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bir Stacked AutoEncoder oluşturup kullanıcının bir filme kaç puan vereceğini tahmin edeceğiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie's id, name, contents 3883 \n",
      "    0                        1                             2\n",
      "0  1         Toy Story (1995)   Animation|Children's|Comedy\n",
      "1  2           Jumanji (1995)  Adventure|Children's|Fantasy\n",
      "2  3  Grumpier Old Men (1995)                Comedy|Romance \n",
      "\n",
      "user's id, sex, age, job, zip code 6040 \n",
      "    0  1   2   3      4\n",
      "0  1  F   1  10  48067\n",
      "1  2  M  56  16  70072\n",
      "2  3  M  25  15  55117 \n",
      "\n",
      "user's id, movie's id, ratings(1-5), timesteps 1000209 \n",
      "    0     1  2          3\n",
      "0  1  1193  5  978300760\n",
      "1  1   661  3  978302109\n",
      "2  1   914  3  978301968 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing the dataset\n",
    "# default seperatör ',' oluyor ama film isimleri virgünl içerebildiği için farklı seperator kullanıyoruz\n",
    "# sütun başlıkları olmadığı için None, bazı film isimleri utf-8'de olmayan özel karakterler içeriyor\n",
    "movies = pd.read_csv(\"data/BM/ml-1m/movies.dat\", sep = \"::\", header = None, engine = \"python\", encoding = \"latin-1\")\n",
    "users = pd.read_csv(\"data/BM/ml-1m/users.dat\", sep = \"::\", header = None, engine = \"python\", encoding = \"latin-1\")\n",
    "ratings = pd.read_csv(\"data/BM/ml-1m/ratings.dat\", sep = \"::\", header = None, engine = \"python\", encoding = \"latin-1\")\n",
    "print(\"movie\\'s id, name, contents\", len(movies), \"\\n\", movies[:3], \"\\n\")\n",
    "print(\"user's id, sex, age, job, zip code\", len(users), \"\\n\", users[:3], \"\\n\")\n",
    "print(\"user's id, movie's id, ratings(1-5), timesteps\", len(ratings), \"\\n\", ratings[:3], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79999 19999\n",
      "[[        1         2         3 876893171]\n",
      " [        1         3         4 878542960]\n",
      " [        1         4         3 876893119]]\n",
      "[[        1        10         3 875693118]\n",
      " [        1        12         5 878542960]\n",
      " [        1        14         5 874965706]]\n"
     ]
    }
   ],
   "source": [
    "# preparing train and testset\n",
    "# bu klasörde 100k veri k=5 fold cros validation için 5e bölünmüş durumda base=train test=test, ama sadece birini kullanacağız\n",
    "training_set = pd.read_csv(\"data/BM/ml-100k/u1.base\", delimiter = \"\\t\")\n",
    "training_set = np.array(training_set, dtype = \"int\")\n",
    "test_set = pd.read_csv(\"data/BM/ml-100k/u1.test\", delimiter = \"\\t\")\n",
    "test_set = np.array(test_set, dtype = \"int\")\n",
    "print(len(training_set), len(test_set))\n",
    "print(training_set[:3])\n",
    "print(test_set[:3]) # yukarıdaki ratings'in aynısı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943 1682\n"
     ]
    }
   ],
   "source": [
    "# Kişiler için matrisler oluşturacağız hangi tarz filmleri beğendiklerine yönelik, max id alsak users sayısını buluruz\n",
    "nb_users = max(max(training_set[:, 0]), max(test_set[:, 0]))\n",
    "nb_movies = max(max(training_set[:, 1]), max(test_set[:, 1]))\n",
    "print(nb_users, nb_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM matris verisi istediği için users are rows & movies are columns olan matrixler oluşturuyoruz\n",
    "def convert(data):\n",
    "    new_data = list()\n",
    "    for id_users in range(1, nb_users+1):\n",
    "        id_movies = data[:, 1][data[:, 0] == id_users] # data[rows][condition], kullanıcının oyladığı filmleri aldık\n",
    "        id_ratings = data[:, 2][data[:, 0] == id_users] # puanlamadığı filmler 0'lar ile doldurulacak\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[id_movies-1] = id_ratings\n",
    "        new_data.append(list(ratings)) # torch expects list of lists\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 3.0, 4.0, 3.0, 3.0, 0.0, 4.0, 1.0, 5.0, 0.0, 2.0, 0.0, 5.0, 0.0, 5.0, 5.0, 0.0, 4.0, 5.0, 0.0, 1.0, 4.0, 0.0, 0.0, 4.0, 3.0, 0.0, 4.0, 1.0, 3.0, 0.0, 5.0, 0.0, 2.0, 1.0, 0.0, 2.0, 3.0, 0.0, 3.0,\n"
     ]
    }
   ],
   "source": [
    "training_set = convert(training_set)\n",
    "test_set = convert(test_set)\n",
    "print(str(training_set[0])[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 3., 4.,  ..., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Converting the data into Torch tensors\n",
    "# Tensorler(multi dimensional matrix with single type) npArray'lerden daha etkili ve hızlı işlem yapılabilmesini sağlıyorlar\n",
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)\n",
    "print(training_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture of the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, ):\n",
    "        super(SAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_movies, 20) # ilk hidden layerin 20 nodesi olacak, bu veri setine 20 uygunmuş\n",
    "        self.fc2 = nn.Linear(20, 10) # ikinci saklı katmanda 10 node olacak, önceki katmanla full connection için 20 yazıldı\n",
    "        self.fc3 = nn.Linear(10, 20) # encoder kısmı bitti decoder kısmınıda aynı yapalım\n",
    "        self.fc4 = nn.Linear(20, nb_movies) # nasıl başladıysa öyle sonlansın simetrik bir ağ olsun\n",
    "        self.activation = nn.Sigmoid() # başka aktivasyon fonksiyonları da seçilebilir\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae = SAE()\n",
    "criterion = nn.MSELoss() # loss func\n",
    "# başka optimizerlar da seçilebilir, lr=learningRate, weight_decat = reduces lr after each epoch and improve the model\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1Loss: 1.7485072011386338\n",
      "Epoch: 2Loss: 1.0961391585681826\n",
      "Epoch: 3Loss: 1.0531720833133846\n",
      "Epoch: 4Loss: 1.038160269860717\n",
      "Epoch: 5Loss: 1.030888710324606\n",
      "Epoch: 6Loss: 1.0264676410229745\n",
      "Epoch: 7Loss: 1.0235527551521468\n",
      "Epoch: 8Loss: 1.0220229222498596\n",
      "Epoch: 9Loss: 1.0208120744092708\n",
      "Epoch: 10Loss: 1.019512248354153\n",
      "Epoch: 11Loss: 1.0187425976871562\n",
      "Epoch: 12Loss: 1.0184008890246776\n",
      "Epoch: 13Loss: 1.0178788119077617\n",
      "Epoch: 14Loss: 1.0174358976060385\n",
      "Epoch: 15Loss: 1.0172821987702128\n",
      "Epoch: 16Loss: 1.0168427928875743\n",
      "Epoch: 17Loss: 1.0168167989185848\n",
      "Epoch: 18Loss: 1.0165568214256753\n",
      "Epoch: 19Loss: 1.0163935127994115\n",
      "Epoch: 20Loss: 1.0161844462347747\n",
      "Epoch: 21Loss: 1.01609627164655\n",
      "Epoch: 22Loss: 1.0158542801575348\n",
      "Epoch: 23Loss: 1.0158134859802004\n",
      "Epoch: 24Loss: 1.015778259771542\n",
      "Epoch: 25Loss: 1.0155341732272432\n",
      "Epoch: 26Loss: 1.015689520344161\n",
      "Epoch: 27Loss: 1.0154221027242052\n",
      "Epoch: 28Loss: 1.0152185463786298\n",
      "Epoch: 29Loss: 1.0134677975765949\n",
      "Epoch: 30Loss: 1.011248689447274\n",
      "Epoch: 31Loss: 1.0099063290015236\n",
      "Epoch: 32Loss: 1.0077616909878302\n",
      "Epoch: 33Loss: 1.0077983532453898\n",
      "Epoch: 34Loss: 1.0032270882891055\n",
      "Epoch: 35Loss: 1.0030146110292932\n",
      "Epoch: 36Loss: 1.0002458614037353\n",
      "Epoch: 37Loss: 0.9965461341719716\n",
      "Epoch: 38Loss: 0.9942404747195641\n",
      "Epoch: 39Loss: 0.9941072993301711\n",
      "Epoch: 40Loss: 0.9928970090877954\n",
      "Epoch: 41Loss: 0.9920356072556301\n",
      "Epoch: 42Loss: 0.9854583435907079\n",
      "Epoch: 43Loss: 0.9865760274286728\n",
      "Epoch: 44Loss: 0.9815889926359588\n",
      "Epoch: 45Loss: 0.9814359942034153\n",
      "Epoch: 46Loss: 0.978836890705219\n",
      "Epoch: 47Loss: 0.9781605495693154\n",
      "Epoch: 48Loss: 0.975182133402695\n",
      "Epoch: 49Loss: 0.972624240591856\n",
      "Epoch: 50Loss: 0.9694380577050556\n",
      "Epoch: 51Loss: 0.9703289929204556\n",
      "Epoch: 52Loss: 0.9671674011179084\n",
      "Epoch: 53Loss: 0.965265354652881\n",
      "Epoch: 54Loss: 0.9632711163284877\n",
      "Epoch: 55Loss: 0.9638322439084875\n",
      "Epoch: 56Loss: 0.9599016011915423\n",
      "Epoch: 57Loss: 0.9591290823099761\n",
      "Epoch: 58Loss: 0.9569117929821981\n",
      "Epoch: 59Loss: 0.955932928211285\n",
      "Epoch: 60Loss: 0.9547701893540932\n",
      "Epoch: 61Loss: 0.9538155228626614\n",
      "Epoch: 62Loss: 0.952312058338782\n",
      "Epoch: 63Loss: 0.9517674709103755\n",
      "Epoch: 64Loss: 0.9507680896133498\n",
      "Epoch: 65Loss: 0.9506239508812659\n",
      "Epoch: 66Loss: 0.9489481193039526\n",
      "Epoch: 67Loss: 0.9486753029269319\n",
      "Epoch: 68Loss: 0.9472455229302577\n",
      "Epoch: 69Loss: 0.9467206282632855\n",
      "Epoch: 70Loss: 0.9454185010447015\n",
      "Epoch: 71Loss: 0.9454827885498203\n",
      "Epoch: 72Loss: 0.9443356538635604\n",
      "Epoch: 73Loss: 0.9443914797140267\n",
      "Epoch: 74Loss: 0.9424805555435337\n",
      "Epoch: 75Loss: 0.9430787504606369\n",
      "Epoch: 76Loss: 0.9411311331616454\n",
      "Epoch: 77Loss: 0.9420330373070873\n",
      "Epoch: 78Loss: 0.940098007923738\n",
      "Epoch: 79Loss: 0.9411008400151338\n",
      "Epoch: 80Loss: 0.9390595354414133\n",
      "Epoch: 81Loss: 0.94001444528845\n",
      "Epoch: 82Loss: 0.9378784104890555\n",
      "Epoch: 83Loss: 0.9387793384395493\n",
      "Epoch: 84Loss: 0.9372452023507959\n",
      "Epoch: 85Loss: 0.9381615469711321\n",
      "Epoch: 86Loss: 0.9363375491435142\n",
      "Epoch: 87Loss: 0.9373362473077432\n",
      "Epoch: 88Loss: 0.935813658600495\n",
      "Epoch: 89Loss: 0.9365222197651614\n",
      "Epoch: 90Loss: 0.9346239031972743\n",
      "Epoch: 91Loss: 0.9354548723395776\n",
      "Epoch: 92Loss: 0.9337049349896023\n",
      "Epoch: 93Loss: 0.9353586893529366\n",
      "Epoch: 94Loss: 0.9331350287250477\n",
      "Epoch: 95Loss: 0.9337225619792682\n",
      "Epoch: 96Loss: 0.9325879220746378\n",
      "Epoch: 97Loss: 0.9332902043980413\n",
      "Epoch: 98Loss: 0.9316994807296656\n",
      "Epoch: 99Loss: 0.9324507826643039\n",
      "Epoch: 100Loss: 0.9306418786148581\n",
      "Epoch: 101Loss: 0.9312204256195856\n",
      "Epoch: 102Loss: 0.9299203588374433\n",
      "Epoch: 103Loss: 0.9304096684703992\n",
      "Epoch: 104Loss: 0.9292580930874955\n",
      "Epoch: 105Loss: 0.9293906359496855\n",
      "Epoch: 106Loss: 0.9279810753939243\n",
      "Epoch: 107Loss: 0.9287838503236175\n",
      "Epoch: 108Loss: 0.9275106643440083\n",
      "Epoch: 109Loss: 0.9278819262917327\n",
      "Epoch: 110Loss: 0.9267763624916421\n",
      "Epoch: 111Loss: 0.927178288567399\n",
      "Epoch: 112Loss: 0.9260017110257631\n",
      "Epoch: 113Loss: 0.9262096692735333\n",
      "Epoch: 114Loss: 0.9251753028939478\n",
      "Epoch: 115Loss: 0.9256316946770055\n",
      "Epoch: 116Loss: 0.9246975177110712\n",
      "Epoch: 117Loss: 0.9251116213324212\n",
      "Epoch: 118Loss: 0.9241800038161427\n",
      "Epoch: 119Loss: 0.9247765196523767\n",
      "Epoch: 120Loss: 0.92376391650869\n",
      "Epoch: 121Loss: 0.9238749384444893\n",
      "Epoch: 122Loss: 0.9231454824412676\n",
      "Epoch: 123Loss: 0.9235160153789157\n",
      "Epoch: 124Loss: 0.9224019076348435\n",
      "Epoch: 125Loss: 0.9228282461652556\n",
      "Epoch: 126Loss: 0.9219096653411697\n",
      "Epoch: 127Loss: 0.9220873611590005\n",
      "Epoch: 128Loss: 0.921297501288451\n",
      "Epoch: 129Loss: 0.9216267607966102\n",
      "Epoch: 130Loss: 0.920604114322488\n",
      "Epoch: 131Loss: 0.9205531421846002\n",
      "Epoch: 132Loss: 0.9202675025346548\n",
      "Epoch: 133Loss: 0.9207131605031783\n",
      "Epoch: 134Loss: 0.9198821163336598\n",
      "Epoch: 135Loss: 0.9199571877268333\n",
      "Epoch: 136Loss: 0.9192134774179207\n",
      "Epoch: 137Loss: 0.9193303195255853\n",
      "Epoch: 138Loss: 0.9189494268275497\n",
      "Epoch: 139Loss: 0.9190746271248564\n",
      "Epoch: 140Loss: 0.9185485681209742\n",
      "Epoch: 141Loss: 0.9183406935984749\n",
      "Epoch: 142Loss: 0.9181777230822535\n",
      "Epoch: 143Loss: 0.9182633068426355\n",
      "Epoch: 144Loss: 0.9176604707417609\n",
      "Epoch: 145Loss: 0.9175523410484054\n",
      "Epoch: 146Loss: 0.916984546856669\n",
      "Epoch: 147Loss: 0.9172109194300617\n",
      "Epoch: 148Loss: 0.9169222513630398\n",
      "Epoch: 149Loss: 0.9173383426351697\n",
      "Epoch: 150Loss: 0.9167493393140049\n",
      "Epoch: 151Loss: 0.9167608121412152\n",
      "Epoch: 152Loss: 0.9163710949455601\n",
      "Epoch: 153Loss: 0.9163750433557012\n",
      "Epoch: 154Loss: 0.9161184353167771\n",
      "Epoch: 155Loss: 0.9159795842036121\n",
      "Epoch: 156Loss: 0.915533474789517\n",
      "Epoch: 157Loss: 0.9155133064956108\n",
      "Epoch: 158Loss: 0.9153453608499741\n",
      "Epoch: 159Loss: 0.9150299829589615\n",
      "Epoch: 160Loss: 0.9150729828852564\n",
      "Epoch: 161Loss: 0.9149565096208857\n",
      "Epoch: 162Loss: 0.9147536832933855\n",
      "Epoch: 163Loss: 0.9147352731406969\n",
      "Epoch: 164Loss: 0.9144409244280228\n",
      "Epoch: 165Loss: 0.9142020785997622\n",
      "Epoch: 166Loss: 0.9141969749331987\n",
      "Epoch: 167Loss: 0.9140162010380314\n",
      "Epoch: 168Loss: 0.9139891479278958\n",
      "Epoch: 169Loss: 0.9137481440637487\n",
      "Epoch: 170Loss: 0.9137497225530583\n",
      "Epoch: 171Loss: 0.9132519126461985\n",
      "Epoch: 172Loss: 0.9134154961035033\n",
      "Epoch: 173Loss: 0.9130654734761949\n",
      "Epoch: 174Loss: 0.913172077075425\n",
      "Epoch: 175Loss: 0.9128916505222803\n",
      "Epoch: 176Loss: 0.9129876254554572\n",
      "Epoch: 177Loss: 0.9124795176868252\n",
      "Epoch: 178Loss: 0.9126701822503506\n",
      "Epoch: 179Loss: 0.9125857110898128\n",
      "Epoch: 180Loss: 0.9124320107308432\n",
      "Epoch: 181Loss: 0.9124731955657006\n",
      "Epoch: 182Loss: 0.912322514622364\n",
      "Epoch: 183Loss: 0.9122291887092625\n",
      "Epoch: 184Loss: 0.9121530082454126\n",
      "Epoch: 185Loss: 0.911786994528581\n",
      "Epoch: 186Loss: 0.9118907208242941\n",
      "Epoch: 187Loss: 0.9115629647372552\n",
      "Epoch: 188Loss: 0.9115650969864424\n",
      "Epoch: 189Loss: 0.9112424825249776\n",
      "Epoch: 190Loss: 0.9113432212152389\n",
      "Epoch: 191Loss: 0.9109754692454007\n",
      "Epoch: 192Loss: 0.9110264141602773\n",
      "Epoch: 193Loss: 0.9105994104732872\n",
      "Epoch: 194Loss: 0.9105935685038894\n",
      "Epoch: 195Loss: 0.9104236660724625\n",
      "Epoch: 196Loss: 0.9103219663309172\n",
      "Epoch: 197Loss: 0.9100146083695464\n",
      "Epoch: 198Loss: 0.9098187432920409\n",
      "Epoch: 199Loss: 0.9094005610941184\n",
      "Epoch: 200Loss: 0.9094364144166274\n"
     ]
    }
   ],
   "source": [
    "np_epoch = 200\n",
    "\n",
    "for epoch in range(1, np_epoch + 1):\n",
    "    train_loss = 0 # there is no loss yet\n",
    "    s = 0. # number of users which didn't rate any movie\n",
    "    \n",
    "    for id_user in range(nb_users):\n",
    "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "        target = input.clone()\n",
    "        if torch.sum(target.data > 0) > 0: # kullanıcı en az bir filmi puanlamışsa\n",
    "            output = sae(input) # input = forward(x)'teki x oluyor\n",
    "            target.require_grad = False\n",
    "            output[target == 0] = 0 # hiç puanlamamış kullanıcılar kayda alınmayacak o yüzden sıfıra eşitleniyorlar\n",
    "            loss = criterion(output, target)\n",
    "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10) # be sure dominator is not zero (+ 1e-10)\n",
    "            loss.backward()\n",
    "            train_loss += np.sqrt(loss.item()*mean_corrector)\n",
    "            s += 1.\n",
    "            optimizer.step()\n",
    "    print(\"Epoch: \" + str(epoch) + \"Loss: \" + str(train_loss/s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.9461880169721086\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "s = 0.\n",
    "\n",
    "for id_user in range(nb_users):\n",
    "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "    target = Variable(test_set[id_user]).unsqueeze(0)\n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "        output = sae(input)\n",
    "        target.require_grad = False\n",
    "        output[target == 0] = 0\n",
    "        loss = criterion(output, target)\n",
    "        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "        test_loss += np.sqrt(loss.item()*mean_corrector)\n",
    "        s += 1.\n",
    "print(\"Loss: \" + str(test_loss/s))\n",
    "# 5 yıldızlı puanlama sisteminde 0.94 1 yıldıza denk gelir. 4 yıldızlık bir film için +-1 hata payı vardır. 3,4,5 olabilir. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sınıfa predict için eklenebilir<br>\n",
    "<br>\n",
    "def predict(self, x): # x: visible nodes<br>\n",
    "        x = self.forward(x)<br>\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
