{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qKr8tClmBvvF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, SpatialDropout1D, LSTM\n",
    "from keras.models import load_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Ncf_h-t0B2Km"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               cumle sinif\n",
      "0  koku çok kız çünkü ayva ben tek arkadaş  insan...   kiz\n",
      "1                 hayır çok sev dizi yani aşırı kız    kiz\n",
      "2  buna kız de evet ona kız ney kız kızneg sen ma...   kiz\n",
      "3                                   kız ve çok üzül    kiz\n",
      "4        kız bu geç ama aynı zaman kır bu nasıl geç    kiz\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/RNN - LSTM/LSTMData.txt')\n",
    "data.columns = ['cumle', 'sinif']\n",
    "data = data[['cumle', 'sinif']]\n",
    "print(data[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0reKvMGDH9R2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1094, 1, 57, 281, 1361, 4, 173, 104, 22, 17, 21, 23, 19, 173, 104, 712, 180], [258, 1, 49, 192, 259, 154, 57]]\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0 1094    1   57  281 1361    4  173  104   22\n",
      "   17   21   23   19  173  104  712  180]\n"
     ]
    }
   ],
   "source": [
    "# Veri setimizin işlenebilmesi için text verileri numaralara çevirmemiz gerekir\n",
    "# Keras bu işlem için hazır bir mekanizma sunmaktadır\n",
    "# Tokenizer sınıfı data içerisinde verilen cümleleri analiz ederek kelimelerin sıklıklarını hesaplar\n",
    "# Parameter: num_words = En sık geçen 25000 kelimeye odaklan, diğerleri önemli değil demek\n",
    "tokenizer = Tokenizer(split = ' ', num_words = 25000)\n",
    "# Her bir kelimenin sıklığını(frekansını) hesaplar\n",
    "tokenizer.fit_on_texts(data['cumle'].values)\n",
    "# Tüm cümleler tam sayı dizisine dönüştürülür\n",
    "X = tokenizer.texts_to_sequences(data['cumle'].values)\n",
    "print(X[0:2])\n",
    "# Bütün metinlerimiz 400 sütundan oluşan bir dizi ile temsil edilecek\n",
    "# Çok kısa metinler 0'lar ile doldurulacak, çok uzun metinler ise kesilecek\n",
    "X = pad_sequences(X,maxlen=400)\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 937,
     "status": "ok",
     "timestamp": 1525800288690,
     "user": {
      "displayName": "Emrah Tema",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102019531740135725055"
     },
     "user_tz": -180
    },
    "id": "xSPPtGNVIDd9",
    "outputId": "6ded9075-b036-4d34-e3f0-2751953ca39b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Yapay sinir ağı modelimiz\n",
    "embed_dim = 128\n",
    "lstm_out = 128\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    # Her bir kelimenin temsil edileceği vektör boyutu. Bu örnek için her bir kelime 128 boyutunda\n",
    "    # bir vektör ile temsil edilir.\n",
    "    model.add(Embedding(25000, embed_dim, input_length = X.shape[1]))\n",
    "    model.add(SpatialDropout1D(rate = 0.2))\n",
    "    model.add(LSTM(lstm_out, dropout = 0.2, recurrent_dropout = 0.2))\n",
    "    model.add(Dense(4, activation = 'softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "  # Çıktılarımızı kategorik hale getirdik (Opsiyonel)\n",
    "Y = pd.get_dummies(data['sinif']).values\n",
    "Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 517821,
     "status": "ok",
     "timestamp": 1525804440484,
     "user": {
      "displayName": "Emrah Tema",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102019531740135725055"
     },
     "user_tz": -180
    },
    "id": "0y9tU9U_IUqN",
    "outputId": "c9e06f88-406d-43e7-e7e9-7d4f83745474"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " - 39s - loss: 1.1542 - acc: 0.5051\n",
      "Epoch 2/5\n",
      " - 42s - loss: 0.4053 - acc: 0.8872\n",
      "Epoch 3/5\n",
      " - 41s - loss: 0.1368 - acc: 0.9608\n",
      "Epoch 4/5\n",
      " - 40s - loss: 0.0710 - acc: 0.9819\n",
      "Epoch 5/5\n",
      " - 40s - loss: 0.0491 - acc: 0.9888\n",
      "score: 0.92\n"
     ]
    }
   ],
   "source": [
    "# Verinin %90'i train, %10'si test verisi olacak şekilde ayrılacak\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1, random_state = 1)\n",
    "\n",
    "model = build_model()\n",
    "# Oluşturulan model train verileri ile eğitilir\n",
    "# Burada yapay sinir ağını eğitmeye başlıyoruz\n",
    "# nb_epoch: İterasyon sayısı\n",
    "model.fit(X_train, Y_train, epochs = 5, batch_size = 32, verbose = 2)\n",
    "# Train verileri ile model eğitildikten sonra test dataları ile doğruluk oranlarına bakılır.\n",
    "score = model.evaluate(X_test, Y_test, verbose = 2)\n",
    "print(\"score: %.2f\" % (score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 844,
     "status": "ok",
     "timestamp": 1525773095097,
     "user": {
      "displayName": "Emrah Tema",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102019531740135725055"
     },
     "user_tz": -180
    },
    "id": "40uzHZ9xIhEm",
    "outputId": "7f23a0f6-cd3c-462b-a6c7-92489b9fbb1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.85069966 0.01189544 0.01280428 0.1246006 ]\n",
      "Kızmak\n"
     ]
    }
   ],
   "source": [
    "# Çıktılarımız => 0: Kızmak, 1: Korkmak, 2: Mutluluk, 3:Üzüntü\n",
    "outputs = [\"Kızmak\", \"Korkmak\", \"Mutluluk\", \"Üzüntü\"]\n",
    "\n",
    "# Verilen örnekler Tokenizer yapısı ile tam sayı dizisine dönüştürülür\n",
    "# Daha sonra eğitilen modele sırayla verilerek anlam analizi sonuçları elde edilir\n",
    "# Her Cümlenin yüzde kaç olumlu ve olumsuz olduğuna dair bilgiler çıktı olarak verilir\n",
    "\n",
    "my_text = [\"çok kız ne kadar ayıp şey\"]\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(my_text)\n",
    "data = pad_sequences(sequences, maxlen = 400)\n",
    "predictions = model.predict(data)\n",
    "print(predictions[0])\n",
    "predictions = list(predictions)\n",
    "print(outputs[predictions.index(max(predictions))])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "LSTM_Test.ipynb adlı dosyanın kopyası",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
