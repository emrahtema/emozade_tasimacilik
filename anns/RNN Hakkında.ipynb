{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN bütün verileri işlerken ağırlıklar belirleyip ağın son halini oluşturur, bu yüzden ANN için long term memory tanımlaması yapılabilir, çünkü bütün öğrendikleriyle ağırlık belirliyor. RNN ise short term memory olarak tanımlanır çünkü bütün veriyle ağırlık belirlemek yerine birkaç gözlem öncesine kadar dayanan verilere göre ağırlıkları şekillendirir. Ben ANN'yi öğrendiğim için şuan RNN ile kıyaslama yapabiliyorum çünkü kısa süre önce öğrendiğim bir bilgi var ve bu bilgiyi kullanıyorum, RNN sayesinde de nöronlar aynen bunu yapıyor, bir sonraki katmandaki nöronlara bağlantıları olmasının yanında kendilerine de bağlantıları vardır ve bir önceki öğrendiklerini hatırlayarak yeni şeyleri öğrenmede ilişkilendirebilirler.<br>\n",
    "Birkaç şekilde çalışabilirler, one to many. Örnek olarak bir eylemde bulunan köpek resmini analiz ederiz ve sonuç olarak \"Siyah ve beyaz renkli köpek bir demirin üzerinden atlıyor\" gibi bir sürü kelimeden oluşan bir sonuç alabiliriz, yani bir sürü çıktının birleşiminden cümle oluşmuş.<br>\n",
    "Bir diğeri ise many to one, örnek olarak bir tane cümle veririz ve bu cümleyle ilgili bir sonuç alırız.<br>\n",
    "Many to many, buna örnek olarak da dil çeviri sistemi örnek verilebilir. Bir dilden diğer dile çevrilmesi gereken bir sürü kelime vardır, yani çoktan çoğa sonuç alınır ve her bir kelime kendisinden sonra gelecek bir kelimeyi şekillendireceğinden bu da short terme örnek olur. Örneğin Türkçe'den İngilizce'ye çevirdiğimizde \"kız geldi ve ona hediye verdiler\", \"girl came and they gave a present to her\" olarak çevrilir. \"kız\" kelimesi \"girl\" olarak çevrildi ve \"ona\" kelimesi İngilizce'de çevrilirken kıza göre çevrildi. Yani \"girl\" tespit edildiği için short term memory kullanılarak \"ona\"'ya karşılık gelmesi gereken kelimenin \"him\" değil de \"her\" olacağına ağ karar verebiliyor. Başka bir örnek olması açısından bir video verimiz olsun ve videoda gerçekleşen olay hakkında yorum yapabilen bir ağ kurmak istiyoruz, işte bunda da RNN kullanmak gerekiyor çünkü videoda diyelimki 25 kare var, her bir karede var olan olayı ağ tespit eder ve aynı zamanda bu karenin kendisinden önceki karelerde tespit ettiği şeylere dayanarak olayı yorumlayabilir. 25 kare boyunca ayakları hareket eden ve konumu değişen bir kedi hakkında koşuyor sonucuna varabilir. Bu da short term memory'e örnektir, önceki karelerdeki çıkarımlarına dayanıyor çünkü. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Problemi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN'nin yapısından ötürü kaynaklanan bir gradient descent problemi mevcuttur. Bu yapıdaki bir ağda back propagation yapıldığında bazı nöronlar iyi bir şekilde eğitilirken bazı nöronlar iyi eğitilemiyor, ağırlıkları düzgünce güncellenemiyor ve güvenilemeyen bir ağ ortaya çıkıyor. Bunun çözümü için normal ağlarda kullandığımızdan farklı yapıda back propagation yapılmalıdır. Bunun için:<br>\n",
    "<b>Exploding Gradient</b><br>\n",
    "-Truncated Backpropagation<br>\n",
    "-Penalties<br>\n",
    "-Gradient Clipping<br>\n",
    "<b>Vanishing Gradient</b><br>\n",
    "-Weight Initialization<br>\n",
    "-Echo State Networks<br>\n",
    "-Long Short-Term Memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
